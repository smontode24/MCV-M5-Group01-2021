Successfully registered 'ds_test'!
Dataset loaded...
Loading model pretrained weights...
[32m[03/26 19:05:24 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): Res5ROIHeads(
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=2048, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=2048, out_features=320, bias=True)
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (deconv): ConvTranspose2d(2048, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[03/26 19:05:52 d2.data.build]: [0mRemoved 0 images with no usable annotations. 2642 images left.
[32m[03/26 19:05:52 d2.data.build]: [0mDistribution of instances among all 3 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   person   | 3347         |  bicycle   | 0            |    car     | 8029         |
|            |              |            |              |            |              |
|   total    | 11376        |            |              |            |              |[0m
[32m[03/26 19:05:52 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/26 19:05:52 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/26 19:05:52 d2.data.common]: [0mSerializing 2642 elements to byte tensors and concatenating them all ...
[32m[03/26 19:05:52 d2.data.common]: [0mSerialized dataset takes 4.33 MiB
Checkpoint loaded...
[32m[03/26 19:05:52 d2.evaluation.coco_evaluation]: [0m'ds_test' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...
[5m[31mWARNING[0m [32m[03/26 19:05:52 d2.data.datasets.coco]: [0mUsing previously cached COCO format annotations at '/home/group01/W4_detectron2/pretrained/test_output/ds_test_coco_format.json'. You need to clear the cache file if your dataset has been modified.
[32m[03/26 19:06:20 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/26 19:06:20 d2.data.common]: [0mSerializing 2642 elements to byte tensors and concatenating them all ...
[32m[03/26 19:06:20 d2.data.common]: [0mSerialized dataset takes 4.33 MiB
[32m[03/26 19:06:20 d2.evaluation.evaluator]: [0mStart inference on 2642 images
[32m[03/26 19:06:24 d2.evaluation.evaluator]: [0mInference done 11/2642. 0.2159 s / img. ETA=0:10:38
[32m[03/26 19:06:29 d2.evaluation.evaluator]: [0mInference done 29/2642. 0.2223 s / img. ETA=0:11:51
[32m[03/26 19:06:34 d2.evaluation.evaluator]: [0mInference done 45/2642. 0.2265 s / img. ETA=0:12:48
[32m[03/26 19:06:39 d2.evaluation.evaluator]: [0mInference done 60/2642. 0.2290 s / img. ETA=0:13:15
[32m[03/26 19:06:44 d2.evaluation.evaluator]: [0mInference done 75/2642. 0.2305 s / img. ETA=0:13:28
[32m[03/26 19:06:49 d2.evaluation.evaluator]: [0mInference done 90/2642. 0.2315 s / img. ETA=0:13:35
[32m[03/26 19:06:54 d2.evaluation.evaluator]: [0mInference done 105/2642. 0.2323 s / img. ETA=0:13:39
[32m[03/26 19:07:00 d2.evaluation.evaluator]: [0mInference done 120/2642. 0.2331 s / img. ETA=0:13:43
[32m[03/26 19:07:05 d2.evaluation.evaluator]: [0mInference done 135/2642. 0.2335 s / img. ETA=0:13:42
[32m[03/26 19:07:10 d2.evaluation.evaluator]: [0mInference done 150/2642. 0.2337 s / img. ETA=0:13:39
[32m[03/26 19:07:15 d2.evaluation.evaluator]: [0mInference done 165/2642. 0.2340 s / img. ETA=0:13:36
[32m[03/26 19:07:20 d2.evaluation.evaluator]: [0mInference done 180/2642. 0.2344 s / img. ETA=0:13:33
[32m[03/26 19:07:25 d2.evaluation.evaluator]: [0mInference done 195/2642. 0.2348 s / img. ETA=0:13:30
[32m[03/26 19:07:30 d2.evaluation.evaluator]: [0mInference done 210/2642. 0.2351 s / img. ETA=0:13:29
[32m[03/26 19:07:35 d2.evaluation.evaluator]: [0mInference done 226/2642. 0.2350 s / img. ETA=0:13:20
[32m[03/26 19:07:40 d2.evaluation.evaluator]: [0mInference done 245/2642. 0.2343 s / img. ETA=0:13:02
[32m[03/26 19:07:46 d2.evaluation.evaluator]: [0mInference done 264/2642. 0.2337 s / img. ETA=0:12:46
[32m[03/26 19:07:51 d2.evaluation.evaluator]: [0mInference done 283/2642. 0.2334 s / img. ETA=0:12:33
[32m[03/26 19:07:56 d2.evaluation.evaluator]: [0mInference done 301/2642. 0.2332 s / img. ETA=0:12:22
[32m[03/26 19:08:01 d2.evaluation.evaluator]: [0mInference done 319/2642. 0.2332 s / img. ETA=0:12:12
[32m[03/26 19:08:06 d2.evaluation.evaluator]: [0mInference done 337/2642. 0.2332 s / img. ETA=0:12:03
[32m[03/26 19:08:11 d2.evaluation.evaluator]: [0mInference done 355/2642. 0.2331 s / img. ETA=0:11:54
[32m[03/26 19:08:17 d2.evaluation.evaluator]: [0mInference done 373/2642. 0.2331 s / img. ETA=0:11:45
[32m[03/26 19:08:22 d2.evaluation.evaluator]: [0mInference done 391/2642. 0.2331 s / img. ETA=0:11:36
[32m[03/26 19:08:27 d2.evaluation.evaluator]: [0mInference done 409/2642. 0.2330 s / img. ETA=0:11:28
[32m[03/26 19:08:32 d2.evaluation.evaluator]: [0mInference done 428/2642. 0.2329 s / img. ETA=0:11:19
[32m[03/26 19:08:37 d2.evaluation.evaluator]: [0mInference done 448/2642. 0.2325 s / img. ETA=0:11:08
[32m[03/26 19:08:42 d2.evaluation.evaluator]: [0mInference done 469/2642. 0.2322 s / img. ETA=0:10:56
[32m[03/26 19:08:47 d2.evaluation.evaluator]: [0mInference done 488/2642. 0.2321 s / img. ETA=0:10:47
[32m[03/26 19:08:52 d2.evaluation.evaluator]: [0mInference done 507/2642. 0.2322 s / img. ETA=0:10:38
[32m[03/26 19:08:57 d2.evaluation.evaluator]: [0mInference done 527/2642. 0.2321 s / img. ETA=0:10:29
[32m[03/26 19:09:02 d2.evaluation.evaluator]: [0mInference done 546/2642. 0.2321 s / img. ETA=0:10:21
[32m[03/26 19:09:08 d2.evaluation.evaluator]: [0mInference done 566/2642. 0.2320 s / img. ETA=0:10:12
[32m[03/26 19:09:13 d2.evaluation.evaluator]: [0mInference done 585/2642. 0.2320 s / img. ETA=0:10:05
[32m[03/26 19:09:18 d2.evaluation.evaluator]: [0mInference done 605/2642. 0.2320 s / img. ETA=0:09:57
[32m[03/26 19:09:23 d2.evaluation.evaluator]: [0mInference done 626/2642. 0.2319 s / img. ETA=0:09:47
[32m[03/26 19:09:28 d2.evaluation.evaluator]: [0mInference done 646/2642. 0.2318 s / img. ETA=0:09:39
[32m[03/26 19:09:33 d2.evaluation.evaluator]: [0mInference done 667/2642. 0.2318 s / img. ETA=0:09:30
[32m[03/26 19:09:38 d2.evaluation.evaluator]: [0mInference done 689/2642. 0.2314 s / img. ETA=0:09:20
[32m[03/26 19:09:43 d2.evaluation.evaluator]: [0mInference done 710/2642. 0.2312 s / img. ETA=0:09:11
[32m[03/26 19:09:49 d2.evaluation.evaluator]: [0mInference done 732/2642. 0.2309 s / img. ETA=0:09:02
[32m[03/26 19:09:54 d2.evaluation.evaluator]: [0mInference done 753/2642. 0.2308 s / img. ETA=0:08:54
[32m[03/26 19:09:59 d2.evaluation.evaluator]: [0mInference done 774/2642. 0.2306 s / img. ETA=0:08:46
[32m[03/26 19:10:04 d2.evaluation.evaluator]: [0mInference done 794/2642. 0.2306 s / img. ETA=0:08:39
[32m[03/26 19:10:09 d2.evaluation.evaluator]: [0mInference done 814/2642. 0.2305 s / img. ETA=0:08:32
[32m[03/26 19:10:14 d2.evaluation.evaluator]: [0mInference done 834/2642. 0.2304 s / img. ETA=0:08:26
[32m[03/26 19:10:19 d2.evaluation.evaluator]: [0mInference done 854/2642. 0.2303 s / img. ETA=0:08:19
[32m[03/26 19:10:24 d2.evaluation.evaluator]: [0mInference done 874/2642. 0.2303 s / img. ETA=0:08:12
[32m[03/26 19:10:29 d2.evaluation.evaluator]: [0mInference done 894/2642. 0.2302 s / img. ETA=0:08:06
[32m[03/26 19:10:34 d2.evaluation.evaluator]: [0mInference done 915/2642. 0.2301 s / img. ETA=0:07:58
[32m[03/26 19:10:39 d2.evaluation.evaluator]: [0mInference done 936/2642. 0.2300 s / img. ETA=0:07:51
[32m[03/26 19:10:45 d2.evaluation.evaluator]: [0mInference done 957/2642. 0.2299 s / img. ETA=0:07:44
[32m[03/26 19:10:50 d2.evaluation.evaluator]: [0mInference done 979/2642. 0.2297 s / img. ETA=0:07:36
[32m[03/26 19:10:55 d2.evaluation.evaluator]: [0mInference done 1000/2642. 0.2296 s / img. ETA=0:07:30
[32m[03/26 19:11:00 d2.evaluation.evaluator]: [0mInference done 1020/2642. 0.2296 s / img. ETA=0:07:23
[32m[03/26 19:11:05 d2.evaluation.evaluator]: [0mInference done 1040/2642. 0.2295 s / img. ETA=0:07:17
[32m[03/26 19:11:10 d2.evaluation.evaluator]: [0mInference done 1061/2642. 0.2295 s / img. ETA=0:07:11
[32m[03/26 19:11:15 d2.evaluation.evaluator]: [0mInference done 1081/2642. 0.2295 s / img. ETA=0:07:05
[32m[03/26 19:11:20 d2.evaluation.evaluator]: [0mInference done 1102/2642. 0.2294 s / img. ETA=0:06:58
[32m[03/26 19:11:25 d2.evaluation.evaluator]: [0mInference done 1123/2642. 0.2293 s / img. ETA=0:06:51
[32m[03/26 19:11:30 d2.evaluation.evaluator]: [0mInference done 1143/2642. 0.2293 s / img. ETA=0:06:46
[32m[03/26 19:11:35 d2.evaluation.evaluator]: [0mInference done 1163/2642. 0.2293 s / img. ETA=0:06:40
[32m[03/26 19:11:41 d2.evaluation.evaluator]: [0mInference done 1183/2642. 0.2293 s / img. ETA=0:06:34
[32m[03/26 19:11:46 d2.evaluation.evaluator]: [0mInference done 1202/2642. 0.2293 s / img. ETA=0:06:29
[32m[03/26 19:11:51 d2.evaluation.evaluator]: [0mInference done 1221/2642. 0.2294 s / img. ETA=0:06:24
[32m[03/26 19:11:56 d2.evaluation.evaluator]: [0mInference done 1239/2642. 0.2295 s / img. ETA=0:06:19
[32m[03/26 19:12:01 d2.evaluation.evaluator]: [0mInference done 1258/2642. 0.2296 s / img. ETA=0:06:14
[32m[03/26 19:12:07 d2.evaluation.evaluator]: [0mInference done 1277/2642. 0.2296 s / img. ETA=0:06:09
[32m[03/26 19:12:12 d2.evaluation.evaluator]: [0mInference done 1297/2642. 0.2296 s / img. ETA=0:06:03
[32m[03/26 19:12:17 d2.evaluation.evaluator]: [0mInference done 1316/2642. 0.2297 s / img. ETA=0:05:58
[32m[03/26 19:12:22 d2.evaluation.evaluator]: [0mInference done 1335/2642. 0.2297 s / img. ETA=0:05:53
[32m[03/26 19:12:27 d2.evaluation.evaluator]: [0mInference done 1355/2642. 0.2298 s / img. ETA=0:05:48
[32m[03/26 19:12:32 d2.evaluation.evaluator]: [0mInference done 1376/2642. 0.2297 s / img. ETA=0:05:41
[32m[03/26 19:12:37 d2.evaluation.evaluator]: [0mInference done 1396/2642. 0.2297 s / img. ETA=0:05:36
[32m[03/26 19:12:43 d2.evaluation.evaluator]: [0mInference done 1416/2642. 0.2296 s / img. ETA=0:05:30
[32m[03/26 19:12:48 d2.evaluation.evaluator]: [0mInference done 1436/2642. 0.2296 s / img. ETA=0:05:24
[32m[03/26 19:12:53 d2.evaluation.evaluator]: [0mInference done 1455/2642. 0.2296 s / img. ETA=0:05:19
[32m[03/26 19:12:58 d2.evaluation.evaluator]: [0mInference done 1475/2642. 0.2297 s / img. ETA=0:05:14
[32m[03/26 19:13:03 d2.evaluation.evaluator]: [0mInference done 1496/2642. 0.2297 s / img. ETA=0:05:08
[32m[03/26 19:13:08 d2.evaluation.evaluator]: [0mInference done 1518/2642. 0.2296 s / img. ETA=0:05:01
[32m[03/26 19:13:14 d2.evaluation.evaluator]: [0mInference done 1539/2642. 0.2296 s / img. ETA=0:04:55
[32m[03/26 19:13:19 d2.evaluation.evaluator]: [0mInference done 1560/2642. 0.2295 s / img. ETA=0:04:49
[32m[03/26 19:13:24 d2.evaluation.evaluator]: [0mInference done 1580/2642. 0.2295 s / img. ETA=0:04:44
[32m[03/26 19:13:29 d2.evaluation.evaluator]: [0mInference done 1600/2642. 0.2295 s / img. ETA=0:04:38
[32m[03/26 19:13:34 d2.evaluation.evaluator]: [0mInference done 1620/2642. 0.2295 s / img. ETA=0:04:33
[32m[03/26 19:13:39 d2.evaluation.evaluator]: [0mInference done 1640/2642. 0.2295 s / img. ETA=0:04:27
[32m[03/26 19:13:44 d2.evaluation.evaluator]: [0mInference done 1661/2642. 0.2294 s / img. ETA=0:04:21
[32m[03/26 19:13:49 d2.evaluation.evaluator]: [0mInference done 1683/2642. 0.2293 s / img. ETA=0:04:15
[32m[03/26 19:13:54 d2.evaluation.evaluator]: [0mInference done 1704/2642. 0.2292 s / img. ETA=0:04:09
[32m[03/26 19:13:59 d2.evaluation.evaluator]: [0mInference done 1725/2642. 0.2292 s / img. ETA=0:04:03
[32m[03/26 19:14:05 d2.evaluation.evaluator]: [0mInference done 1745/2642. 0.2291 s / img. ETA=0:03:58
[32m[03/26 19:14:10 d2.evaluation.evaluator]: [0mInference done 1764/2642. 0.2291 s / img. ETA=0:03:53
[32m[03/26 19:14:15 d2.evaluation.evaluator]: [0mInference done 1780/2642. 0.2293 s / img. ETA=0:03:49
[32m[03/26 19:14:20 d2.evaluation.evaluator]: [0mInference done 1795/2642. 0.2294 s / img. ETA=0:03:46
[32m[03/26 19:14:25 d2.evaluation.evaluator]: [0mInference done 1810/2642. 0.2295 s / img. ETA=0:03:42
[32m[03/26 19:14:30 d2.evaluation.evaluator]: [0mInference done 1825/2642. 0.2296 s / img. ETA=0:03:38
[32m[03/26 19:14:35 d2.evaluation.evaluator]: [0mInference done 1841/2642. 0.2297 s / img. ETA=0:03:35
[32m[03/26 19:14:40 d2.evaluation.evaluator]: [0mInference done 1856/2642. 0.2298 s / img. ETA=0:03:31
[32m[03/26 19:14:45 d2.evaluation.evaluator]: [0mInference done 1872/2642. 0.2299 s / img. ETA=0:03:27
[32m[03/26 19:14:51 d2.evaluation.evaluator]: [0mInference done 1888/2642. 0.2300 s / img. ETA=0:03:23
[32m[03/26 19:14:56 d2.evaluation.evaluator]: [0mInference done 1904/2642. 0.2301 s / img. ETA=0:03:19
[32m[03/26 19:15:01 d2.evaluation.evaluator]: [0mInference done 1921/2642. 0.2302 s / img. ETA=0:03:15
[32m[03/26 19:15:06 d2.evaluation.evaluator]: [0mInference done 1938/2642. 0.2303 s / img. ETA=0:03:10
[32m[03/26 19:15:11 d2.evaluation.evaluator]: [0mInference done 1955/2642. 0.2303 s / img. ETA=0:03:06
[32m[03/26 19:15:16 d2.evaluation.evaluator]: [0mInference done 1971/2642. 0.2304 s / img. ETA=0:03:02
[32m[03/26 19:15:22 d2.evaluation.evaluator]: [0mInference done 1988/2642. 0.2306 s / img. ETA=0:02:57
[32m[03/26 19:15:27 d2.evaluation.evaluator]: [0mInference done 2005/2642. 0.2307 s / img. ETA=0:02:53
[32m[03/26 19:15:32 d2.evaluation.evaluator]: [0mInference done 2023/2642. 0.2307 s / img. ETA=0:02:48
[32m[03/26 19:15:37 d2.evaluation.evaluator]: [0mInference done 2042/2642. 0.2307 s / img. ETA=0:02:43
[32m[03/26 19:15:42 d2.evaluation.evaluator]: [0mInference done 2062/2642. 0.2308 s / img. ETA=0:02:37
[32m[03/26 19:15:47 d2.evaluation.evaluator]: [0mInference done 2083/2642. 0.2308 s / img. ETA=0:02:32
[32m[03/26 19:15:52 d2.evaluation.evaluator]: [0mInference done 2103/2642. 0.2308 s / img. ETA=0:02:26
[32m[03/26 19:15:58 d2.evaluation.evaluator]: [0mInference done 2124/2642. 0.2307 s / img. ETA=0:02:20
[32m[03/26 19:16:03 d2.evaluation.evaluator]: [0mInference done 2142/2642. 0.2308 s / img. ETA=0:02:15
[32m[03/26 19:16:08 d2.evaluation.evaluator]: [0mInference done 2159/2642. 0.2309 s / img. ETA=0:02:11
[32m[03/26 19:16:13 d2.evaluation.evaluator]: [0mInference done 2176/2642. 0.2310 s / img. ETA=0:02:06
[32m[03/26 19:16:18 d2.evaluation.evaluator]: [0mInference done 2193/2642. 0.2311 s / img. ETA=0:02:02
[32m[03/26 19:16:23 d2.evaluation.evaluator]: [0mInference done 2210/2642. 0.2312 s / img. ETA=0:01:57
[32m[03/26 19:16:28 d2.evaluation.evaluator]: [0mInference done 2227/2642. 0.2313 s / img. ETA=0:01:53
[32m[03/26 19:16:34 d2.evaluation.evaluator]: [0mInference done 2244/2642. 0.2314 s / img. ETA=0:01:48
[32m[03/26 19:16:39 d2.evaluation.evaluator]: [0mInference done 2261/2642. 0.2315 s / img. ETA=0:01:44
[32m[03/26 19:16:44 d2.evaluation.evaluator]: [0mInference done 2278/2642. 0.2317 s / img. ETA=0:01:39
[32m[03/26 19:16:49 d2.evaluation.evaluator]: [0mInference done 2295/2642. 0.2318 s / img. ETA=0:01:35
[32m[03/26 19:16:54 d2.evaluation.evaluator]: [0mInference done 2312/2642. 0.2319 s / img. ETA=0:01:30
[32m[03/26 19:17:00 d2.evaluation.evaluator]: [0mInference done 2329/2642. 0.2320 s / img. ETA=0:01:25
[32m[03/26 19:17:05 d2.evaluation.evaluator]: [0mInference done 2347/2642. 0.2320 s / img. ETA=0:01:20
[32m[03/26 19:17:10 d2.evaluation.evaluator]: [0mInference done 2369/2642. 0.2319 s / img. ETA=0:01:14
[32m[03/26 19:17:15 d2.evaluation.evaluator]: [0mInference done 2390/2642. 0.2319 s / img. ETA=0:01:08
[32m[03/26 19:17:20 d2.evaluation.evaluator]: [0mInference done 2411/2642. 0.2318 s / img. ETA=0:01:03
[32m[03/26 19:17:25 d2.evaluation.evaluator]: [0mInference done 2432/2642. 0.2318 s / img. ETA=0:00:57
[32m[03/26 19:17:30 d2.evaluation.evaluator]: [0mInference done 2452/2642. 0.2318 s / img. ETA=0:00:51
[32m[03/26 19:17:36 d2.evaluation.evaluator]: [0mInference done 2473/2642. 0.2317 s / img. ETA=0:00:46
[32m[03/26 19:17:41 d2.evaluation.evaluator]: [0mInference done 2494/2642. 0.2317 s / img. ETA=0:00:40
[32m[03/26 19:17:46 d2.evaluation.evaluator]: [0mInference done 2515/2642. 0.2316 s / img. ETA=0:00:34
[32m[03/26 19:17:51 d2.evaluation.evaluator]: [0mInference done 2536/2642. 0.2316 s / img. ETA=0:00:28
[32m[03/26 19:17:56 d2.evaluation.evaluator]: [0mInference done 2557/2642. 0.2315 s / img. ETA=0:00:23
[32m[03/26 19:18:01 d2.evaluation.evaluator]: [0mInference done 2578/2642. 0.2314 s / img. ETA=0:00:17
[32m[03/26 19:18:06 d2.evaluation.evaluator]: [0mInference done 2597/2642. 0.2314 s / img. ETA=0:00:12
[32m[03/26 19:18:11 d2.evaluation.evaluator]: [0mInference done 2617/2642. 0.2314 s / img. ETA=0:00:06
[32m[03/26 19:18:17 d2.evaluation.evaluator]: [0mInference done 2637/2642. 0.2314 s / img. ETA=0:00:01
[32m[03/26 19:18:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:11:56.004630 (0.271522 s / img per device, on 1 devices)
[32m[03/26 19:18:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:10:10 (0.231414 s / img per device, on 1 devices)
[32m[03/26 19:18:20 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[03/26 19:18:20 d2.evaluation.coco_evaluation]: [0mSaving results to /home/group01/W4_detectron2/pretrained/test_output/coco_instances_results.json
[32m[03/26 19:18:21 d2.evaluation.coco_evaluation]: [0mEvaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.13s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
COCOeval_opt.evaluate() finished in 1.04 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.29 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.570
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.823
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.639
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.690
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.649
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.923
[32m[03/26 19:18:22 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 56.974 | 82.266 | 63.882 | 36.361 | 69.034 | 64.891 |
[32m[03/26 19:18:22 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category   | AP     | category   | AP   | category   | AP     |
|:-----------|:-------|:-----------|:-----|:-----------|:-------|
| person     | 47.076 | bicycle    | nan  | car        | 66.871 |
Loading and preparing results...
DONE (t=1.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
COCOeval_opt.evaluate() finished in 1.26 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.28 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.802
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.598
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.723
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.564
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.578
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.411
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.669
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.817
[32m[03/26 19:18:28 d2.evaluation.coco_evaluation]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 49.363 | 80.222 | 52.606 | 27.125 | 59.809 | 72.313 |
[32m[03/26 19:18:28 d2.evaluation.coco_evaluation]: [0mPer-category segm AP: 
| category   | AP     | category   | AP   | category   | AP     |
|:-----------|:-------|:-----------|:-----|:-----------|:-------|
| person     | 35.007 | bicycle    | nan  | car        | 63.718 |
[32m[03/26 19:18:29 d2.engine.defaults]: [0mEvaluation results for ds_test in csv format:
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: 56.9739,82.2661,63.8823,36.3605,69.0339,64.8910
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/26 19:18:29 d2.evaluation.testing]: [0mcopypaste: 49.3627,80.2220,52.6059,27.1247,59.8086,72.3131
Testing finished...
